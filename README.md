# Robotic World Model Extension for Isaac Lab

[![IsaacSim](https://img.shields.io/badge/IsaacSim-4.5.0-silver.svg)](https://docs.omniverse.nvidia.com/isaacsim/latest/overview.html)
[![Isaac Lab](https://img.shields.io/badge/IsaacLab-2.1.0-silver)](https://isaac-sim.github.io/IsaacLab)
[![Python](https://img.shields.io/badge/python-3.10-blue.svg)](https://docs.python.org/3/whatsnew/3.10.html)
[![Linux platform](https://img.shields.io/badge/platform-linux--64-orange.svg)](https://releases.ubuntu.com/20.04/)
[![Windows platform](https://img.shields.io/badge/platform-windows--64-orange.svg)](https://www.microsoft.com/en-us/)
[![pre-commit](https://img.shields.io/badge/pre--commit-enabled-brightgreen?logo=pre-commit&logoColor=white)](https://pre-commit.com/)
[![License](https://img.shields.io/badge/license-MIT-yellow.svg)](https://opensource.org/license/mit)

## Overview

This repository extends [**Isaac Lab**](https://github.com/isaac-sim/IsaacLab) with environments and training pipelines for [**Robotic World Model (RWM)**](https://sites.google.com/view/roboticworldmodel/home) and related model-based reinforcement learning methods.  
It enables:
- joint training of policies and neural dynamics models in Isaac Lab,
- evaluation of model-based vs. model-free policies,
- visualization of autoregressive “imagination” rollouts from learned dynamics.


![Robotic World Model](rwm.png)

**Paper**: [Robotic World Model: A Neural Network Simulator for Robust Policy Optimization in Robotics](https://arxiv.org/abs/2501.10100)  
**Project Page**: [https://sites.google.com/view/roboticworldmodel](https://sites.google.com/view/roboticworldmodel)

**Authors**: [Chenhao Li](https://breadli428.github.io/), [Andreas Krause](https://las.inf.ethz.ch/krausea), [Marco Hutter](https://rsl.ethz.ch/the-lab/people/person-detail.MTIxOTEx.TGlzdC8yNDQxLC0xNDI1MTk1NzM1.html)  
**Affiliation**: [ETH AI Center](https://ai.ethz.ch/), [Learning & Adaptive Systems Group](https://las.inf.ethz.ch/) and [Robotic Systems Lab](https://rsl.ethz.ch/), [ETH Zurich](https://ethz.ch/en.html)


---


## Installation

1. **Install Isaac Lab**  

Follow the official [installation guide](https://isaac-sim.github.io/IsaacLab/main/source/setup/installation/index.html). We recommend using the Conda installation as it simplifies calling Python scripts from the terminal.

2. **Install model-based RSL RL**

Follow the official installation guide of model-based [RSL RL](https://github.com/leggedrobotics/rsl_rl_rwm) for model-based reinforcement learning to replace the `rsl_rl_lib` that comes with Isaac Lab.

3. **Clone this repository** (outside your Isaac Lab directory)

```bash
git clone git@github.com:leggedrobotics/robotic_world_model.git
```

4. **Install the extension** using the Python environment where Isaac Lab is installed

```bash
python -m pip install -e source/mbrl
```

5. **Verify the installation**

```bash
python scripts/reinforcement_learning/rsl_rl/train.py --task Template-Isaac-Velocity-Flat-Anymal-D-Init-v0 --headless
```

---

## World Model Pretraining & Evaluation

Robotic World Model is a model-based reinforcement learning algorithm that learns a dynamics model and a policy concurrently.

### Configure model inputs/outputs

You can configure the model inputs and outputs under `ObservationsCfg_PRETRAIN` in [`AnymalDFlatEnvCfg_PRETRAIN`](source/mbrl/mbrl/tasks/manager_based/locomotion/velocity/config/anymal_d/flat_env_cfg.py).

Available components:
- `SystemStateCfg`: state input and output head
- `SystemActionCfg`: action input
- `SystemExtensionCfg`: continuous privileged output head (e.g. rewards etc.)
- `SystemContactCfg`: binary privileged output head (e.g. contacts)
- `SystemTerminationCfg`: binary privileged output head (e.g. terminations)


### Run dynamics model pretraining:

```bash
python scripts/reinforcement_learning/rsl_rl/train.py \
  --task Template-Isaac-Velocity-Flat-Anymal-D-Pretrain-v0 \
  --headless
```

It trains a PPO policy from scratch, while the induced experience during training is used to train the dynamics model.

### Visualize autoregressive predictions

```bash
python scripts/reinforcement_learning/rsl_rl/visualize.py \
  --task Template-Isaac-Velocity-Flat-Anymal-D-Visualize-v0 \
  --headless \
  --load_run <run_name> \
  --system_dynamics_load_path <dynamics_model_path>
```

It visualizes the learned dynamics model by rolling out the model autoregressively in imagination, conditioned on the actions from the learned policy.
The `dynamics_model_path` should point to the pretrained dynamics model checkpoint (e.g. `model_<iteration>.pt`) inside the saved run directory.

---

## Model-Based Policy Training & Evaluation

Once a dynamics model is pretrained, you can train a model-based policy purely from **imagined rollouts** generated by the learned dynamics.

### Train policy in imagination

```bash
python scripts/reinforcement_learning/rsl_rl/train.py --task Template-Isaac-Velocity-Flat-Anymal-D-Finetune-v0 --headless --load_run <run_name> --system_dynamics_load_path <dynamics_model_path>
```

You can either start the policy from pretrained checkpoints or from scratch by simply omitting the `--load_run` argument.

### Play the learned model-based policy

You can play the learned policies with the original Isaac Lab task registry.

```bash
python scripts/reinforcement_learning/rsl_rl/play.py --task Isaac-Velocity-Flat-Anymal-D-Play-v0 --headless --load_run <run_name>
```

## Code Structure

We provide a reference pipeline that enables RWM on ANYmal D.

Key files:
- **Environment configurations + dynamics model setup**
  [`flat_env_cfg.py`](source/mbrl/mbrl/tasks/manager_based/locomotion/velocity/config/anymal_d/flat_env_cfg.py).
- **Algorithm configuration + training parameters**
  [`anymal_d_manager_based_mbrl_env.py`](source/mbrl/mbrl/tasks/manager_based/locomotion/velocity/config/anymal_d/envs/anymal_d_manager_based_mbrl_env.py).
- **Imagination rollout logic** (constructs policy observations & rewards from model outputs)
  [`anymal_d_manager_based_mbrl_env`](source/mbrl/mbrl/tasks/manager_based/locomotion/velocity/config/anymal_d/envs/anymal_d_manager_based_mbrl_env.py).
- **Visualization environment + rollout reset**
  [`anymal_d_manager_based_visualize_env.py`](source/mbrl/mbrl/tasks/manager_based/locomotion/velocity/config/anymal_d/envs/anymal_d_manager_based_visualize_env.py).


## Citation
If you find this repository useful for your research, please consider citing:

```text
@article{li2025robotic,
  title={Robotic world model: A neural network simulator for robust policy optimization in robotics},
  author={Li, Chenhao and Krause, Andreas and Hutter, Marco},
  journal={arXiv preprint arXiv:2501.10100},
  year={2025}
}
```
